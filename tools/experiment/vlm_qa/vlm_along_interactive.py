system_prompt = """You are a reasoning and interaction expert evaluating a robotic manipulation benchmark called COIN, which includes tasks that require both object-level reasoning and interaction understanding.

Your goal is to compare two plans for the same task:
- The first plan is the **ground-truth plan** annotated by human experts.
- The second plan is generated by a **model** under evaluation.

Each plan describes a sequence of actions (sometimes including reasoning steps) designed to complete a specific manipulation task in a simulated environment.

Please evaluate the similarity between the two plans along the following axes:

1. **Goal Alignment**: Do both plans aim to complete the same task successfully?
2. **Interaction Accuracy**: Are the object manipulations in both plans consistent in terms of physical interaction and constraints?
3. **Reasoning Similarity**: Do both plans reflect similar understanding of the causal or relational dependencies (e.g., opening a cabinet before fetching an item)?
4. **Step Coverage**: Does the model-generated plan include the core steps from the ground-truth plan?
5. **Action Order**: Is the temporal order of actions preserved or logically consistent?

Then assign a **similarity score** from 0 to 5:
- **5** – Nearly identical in structure, reasoning, and action
- **4** – Strong similarity with only minor deviations
- **3** – Moderate similarity with some omissions or mismatches
- **2** – Weak similarity; only partial alignment with the ground-truth
- **1** – Minimal similarity; most actions are different or misaligned
- **0** – Completely different or irrelevant to the task

Output your evaluation in the following format:
"""

gt_notation = """[
"open the cabinet door",
"pick the apple and put the apple to the marker"
]"""

gpt_notation = """
[
"grasp the handle",
"rotate the handle along the axis",
"release the handle",
"grap the apple",
"put the apple to the marker"
]
"""
