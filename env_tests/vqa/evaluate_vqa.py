#!/usr/bin/env python
"""
Script to evaluate VQA (Visual Question Answering) answers generated by HVLA.
This script compares the model's answers with ground truth answers and calculates accuracy.
"""

import os
import json
import csv
import glob
import re
from collections import defaultdict
import argparse
import pickle as pkl

def extract_env_id_from_path(episode_path):
    """Load env id from single episode path."""
    metadata_path = os.path.join(episode_path, "metadata", "metadata.json")
    # If no metadata.json, return None
    if not os.path.exists(metadata_path):
        return None
    with open(metadata_path, "r") as f:
        metadata = json.load(f)
    return metadata["environment"]["name"]

def load_vqa_result(episode_path):
    """Load VQA result from a JSON file."""
    vqa_files = glob.glob(os.path.join(episode_path, "traj_*_vqa.json"))
    
    if not vqa_files:
        print(f"Warning: No VQA file found in {episode_path}")
        return None
    
    # Use the first matching file if multiple exist
    vqa_file = vqa_files[0]
    
    try:
        with open(vqa_file, 'r') as f:
            data = json.load(f)
        
        # Extract the answer from the JSON structure
        if 'answer' in data and data['answer'] and data['answer']['answer']:
            return data['answer']['answer']
        else:
            print(f"Warning: Could not find 'answer' key in {vqa_file} or the answer is null")
            return None
    except Exception as e:
        print(f"Error loading {vqa_file}: {e}")
        return None


def get_ground_truth_answer(env_id, env_ins_objects):
    """
    Load the ground truth anser from env_ins_objects.pkl
    
    This function should be updated if there's a more direct way to get the ground truth,
    but for now it relies on the environment files.
    """
    # Map from environment ID to the corresponding Python file
    try:
        ground_truth = env_ins_objects.get(env_id, {}).get("answer")
    except Exception as e:
        print(f"Error loading {env_id}: {e}")
        return None
    return ground_truth


def evaluate_vqa_func(evaluation_dir, output_dir):
    """
    Evaluate VQA answers and calculate accuracy for each environment task.
    
    Args:
        evaluation_dir: Directory containing evaluation results
        output_dir: Directory to save the results
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Load env_ins_objects.pkl
    env_ins_objects = pkl.load(open("env_tests/vqa/env_ins_objects.pkl", "rb"))
    
    # Find all episode directories
    episode_dirs = [f for f in os.listdir(evaluation_dir)
              if os.path.isdir(os.path.join(evaluation_dir, f))]
    
    # Group episodes by environment ID
    env_episodes = defaultdict(list)
    for episode_name in episode_dirs:
        env_id = extract_env_id_from_path(os.path.join(evaluation_dir, episode_name))
        if env_id:
            env_episodes[env_id].append(os.path.join(evaluation_dir, episode_name))
    
    # Evaluate each environment
    results = []
    for env_id, episodes in env_episodes.items():
        correct_count = 0
        total_count = 0
        
        ground_truth = get_ground_truth_answer(env_id, env_ins_objects)
        if not ground_truth:
            print(f"Skipping {env_id} due to missing ground truth")
            continue
        
        for episode_dir in episodes:
            model_answer = load_vqa_result(episode_dir)
            
            if model_answer:
                total_count += 1
                if model_answer == ground_truth:
                    correct_count += 1
        
        # Calculate accuracy
        accuracy = (correct_count / total_count * 100) if total_count > 0 else 0
        results.append({
            "Task": env_id,
            "Accuracy": f"{accuracy:.2f}%",
            "Success correct num": correct_count,
            "Total num": total_count
        })
        print(f"Task: {env_id}, Accuracy: {accuracy:.2f}%, Correct: {correct_count}, Total: {total_count}")
    
    # Save results to CSV
    output_file = os.path.join(output_dir, f"{evaluation_dir.split('/')[-1]}.csv")
    with open(output_file, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=["Task", "Accuracy", "Success correct num", "Total num"])
        writer.writeheader()
        writer.writerows(results)
    
    print(f"Results saved to {output_file}")


def main():
    parser = argparse.ArgumentParser(description='Evaluate VQA answers generated by HVLA')
    parser.add_argument('--eval_dir', type=str, 
                        default='/home/wangxianhao/data/project/reasoning/ManiSkill/evaluation/20250509_cogact_30000_gemini_10_400_no_history_image',
                        help='Directory containing evaluation results')
    parser.add_argument('--output_dir', type=str, 
                        default='/home/wangxianhao/data/project/reasoning/ManiSkill/env_tests/vqa_results',
                        help='Directory to save the results')
    
    eval_dir='evaluation/202505010_pi0_470000_gemini_10_400_no_history_image_reverse'
    args = parser.parse_args()
    evaluate_vqa_func(eval_dir, args.output_dir)


if __name__ == "__main__":
    main()
